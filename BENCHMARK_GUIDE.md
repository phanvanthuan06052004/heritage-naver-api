# ğŸ“Š RAG Benchmark System - HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng

## ğŸ¯ Tá»•ng Quan

Há»‡ thá»‘ng benchmark RAG giÃºp Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng cá»§a há»‡ thá»‘ng Retrieval-Augmented Generation thÃ´ng qua cÃ¡c metrics:

- **BLEU**: Äo Ä‘á»™ chÃ­nh xÃ¡c n-gram vá»›i ground truth
- **ROUGE-L**: Äo Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng dá»±a trÃªn Longest Common Subsequence
- **Cosine Similarity**: Äo Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a (TF-IDF vÃ  Semantic Embedding)

---

## ğŸ“ Cáº¥u TrÃºc File

```
heritage-naver-api/
â”œâ”€â”€ benchmark_data.json              # Dá»¯ liá»‡u test cases (input)
â”œâ”€â”€ benchmark_results.json           # Káº¿t quáº£ benchmark (output)
â”‚
â””â”€â”€ src/
    â””â”€â”€ benchmark/
        â”œâ”€â”€ runBenchmark.js          # Script chÃ­nh - Äiá»u phá»‘i toÃ n bá»™ pipeline
        â”œâ”€â”€ ragBenchmark.js          # RAG pipeline - Retrieve + Generate + Evaluate
        â”œâ”€â”€ metrics.js               # TÃ­nh toÃ¡n metrics (BLEU, ROUGE-L, Cosine)
        â””â”€â”€ analyzer.js              # PhÃ¢n tÃ­ch vÃ  bÃ¡o cÃ¡o káº¿t quáº£
```

### ğŸ“„ Chi Tiáº¿t Chá»©c NÄƒng Tá»«ng File

#### 1ï¸âƒ£ `benchmark_data.json` (Input)

**Má»¥c Ä‘Ã­ch**: Chá»©a test cases Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ RAG

**Cáº¥u trÃºc**:

```json
[
  {
    "id": "q1",
    "question": "CÃ¢u há»i vá» di sáº£n vÄƒn hÃ³a",
    "ground_truth": "CÃ¢u tráº£ lá»i chuáº©n (reference answer)",
    "related_docs": ["doc_id_1", "doc_id_2"],
    "expected_context": "Ngá»¯ cáº£nh mong Ä‘á»£i há»‡ thá»‘ng retrieve Ä‘Æ°á»£c"
  }
]
```

**Vai trÃ²**: Cung cáº¥p questions + ground truth Ä‘á»ƒ so sÃ¡nh vá»›i cÃ¢u tráº£ lá»i sinh ra tá»« RAG

---

#### 2ï¸âƒ£ `metrics.js` (Core Calculation)

**Má»¥c Ä‘Ã­ch**: TÃ­nh toÃ¡n cÃ¡c metrics Ä‘Ã¡nh giÃ¡

**Functions chÃ­nh**:

- `calculateBLEU(candidate, reference)`: TÃ­nh BLEU score (n-gram precision vá»›i brevity penalty)
- `calculateROUGEL(candidate, reference)`: TÃ­nh ROUGE-L score (LCS-based F1)
- `calculateCosineSimilarity(text1, text2)`: TÃ­nh Cosine similarity (TF-IDF vectors)
- `calculateSemanticSimilarity(text1, text2, embeddings)`: Cosine similarity trÃªn embeddings
- `evaluateAnswer(generated, groundTruth, options)`: Tá»•ng há»£p táº¥t cáº£ metrics

**Input**: Generated answer + Ground truth answer
**Output**: Object chá»©a BLEU, ROUGE-L, Cosine, Semantic scores

---

#### 3ï¸âƒ£ `ragBenchmark.js` (RAG Pipeline)

**Má»¥c Ä‘Ã­ch**: Thá»±c thi RAG pipeline vÃ  Ä‘Ã¡nh giÃ¡ káº¿t quáº£

**Functions chÃ­nh**:

- `retrieveContext(question, collectionName, topK)`: Query Chroma DB Ä‘á»ƒ láº¥y context
  - Input: Question string, collection name, sá»‘ documents cáº§n retrieve
  - Output: Array of retrieved documents vá»›i content, metadata, distance
- `generateAnswer(question, context)`: Gá»i Naver Chat API Ä‘á»ƒ sinh cÃ¢u tráº£ lá»i
  - Input: Question + retrieved context
  - Output: Generated answer string
- `benchmarkSingleQuestion(testCase, options)`: Pipeline hoÃ n chá»‰nh cho 1 question
  - Steps: Retrieve â†’ Generate â†’ Evaluate â†’ Return result vá»›i metrics
- `benchmarkDataset(testCases, options)`: Cháº¡y benchmark cho toÃ n bá»™ dataset
  - Input: Array of test cases
  - Output: Array of results vá»›i metrics cho tá»«ng question

**Data Flow**:

```
Question â†’ retrieveContext() â†’ Retrieved Docs
                                       â†“
Question + Context â†’ generateAnswer() â†’ Generated Answer
                                       â†“
Generated + Ground Truth â†’ evaluateAnswer() â†’ Metrics
```

---

#### 4ï¸âƒ£ `analyzer.js` (Result Analysis)

**Má»¥c Ä‘Ã­ch**: PhÃ¢n tÃ­ch káº¿t quáº£ vÃ  táº¡o bÃ¡o cÃ¡o chi tiáº¿t

**Functions chÃ­nh**:

- `analyzeSummary(results)`: TÃ­nh toÃ¡n thá»‘ng kÃª tá»•ng quan
  - Averages cá»§a táº¥t cáº£ metrics
  - Quality distribution (Excellent/Good/Fair/Poor/Very Poor)
  - Execution time statistics
- `printSummary(results)`: In ra tÃ³m táº¯t káº¿t quáº£ vá»›i emoji indicators
- `printDetailedResults(results)`: In báº£ng ASCII chi tiáº¿t tá»«ng question
- `printWorstPerformers(results, topN)`: Hiá»ƒn thá»‹ cÃ¡c cÃ¢u há»i cÃ³ Ä‘iá»ƒm tháº¥p nháº¥t
- `generateSuggestions(results)`: ÄÆ°a ra gá»£i Ã½ cáº£i thiá»‡n
  - Retrieval issues (context khÃ´ng Ä‘á»§, khÃ´ng liÃªn quan)
  - Generation issues (cÃ¢u tráº£ lá»i ngáº¯n, khÃ´ng khá»›p ground truth)
  - Performance issues (slow execution)
- `generateFullReport(results)`: Táº¡o bÃ¡o cÃ¡o Ä‘áº§y Ä‘á»§ (gá»i táº¥t cáº£ functions trÃªn)

**Output**: Console logs + exported JSON vá»›i metadata

---

#### 5ï¸âƒ£ `runBenchmark.js` (Main Orchestrator)

**Má»¥c Ä‘Ã­ch**: Äiá»u phá»‘i toÃ n bá»™ pipeline benchmark

**Workflow**:

```
1. Load benchmark_data.json
2. Check Chroma collection status
3. Run benchmarkDataset() tá»« ragBenchmark.js
4. Generate report báº±ng analyzer.js
5. Save results to benchmark_results.json
```

**CLI Options**:

```bash
--input <file>        # Input JSON file (default: benchmark_data.json)
--output <file>       # Output JSON file (default: benchmark_results.json)
--collection <name>   # Chroma collection name (default: heritage_documents)
--topk <number>       # Number of docs to retrieve (default: 5)
--mock                # Mock mode (khÃ´ng gá»i API tháº­t)
--help                # Show help
```

---

## ğŸ”„ Data Flow Pipeline

### BÆ°á»›c 1: Chuáº©n Bá»‹ Dá»¯ Liá»‡u

```
benchmark_data.json â†’ runBenchmark.js (loadBenchmarkData)
```

### BÆ°á»›c 2: Kiá»ƒm Tra Collection

```
runBenchmark.js â†’ ragBenchmark.checkCollectionStatus() â†’ Chroma DB
```

### BÆ°á»›c 3: Thá»±c Thi RAG Pipeline (Má»—i Test Case)

```
Test Case (question, ground_truth)
    â†“
ragBenchmark.benchmarkSingleQuestion()
    â”œâ”€â†’ retrieveContext()
    â”‚       â””â”€â†’ Chroma DB query â†’ Retrieved Documents
    â”‚
    â”œâ”€â†’ generateAnswer(question, context)
    â”‚       â””â”€â†’ Naver Chat API â†’ Generated Answer
    â”‚
    â””â”€â†’ metrics.evaluateAnswer(generated, ground_truth)
            â””â”€â†’ BLEU, ROUGE-L, Cosine, Semantic scores
    â†“
Result Object {
  id, question, ground_truth, generated_answer,
  retrieved_context, metrics, execution_time
}
```

### BÆ°á»›c 4: PhÃ¢n TÃ­ch Káº¿t Quáº£

```
Array of Results â†’ analyzer.js
    â”œâ”€â†’ analyzeSummary() â†’ Statistics
    â”œâ”€â†’ printSummary() â†’ Console output
    â”œâ”€â†’ printDetailedResults() â†’ ASCII table
    â”œâ”€â†’ printWorstPerformers() â†’ Low score analysis
    â””â”€â†’ generateSuggestions() â†’ Improvement recommendations
```

### BÆ°á»›c 5: LÆ°u Káº¿t Quáº£

```
Results + Metadata â†’ exportResults() â†’ benchmark_results.json
```

---

## ğŸš€ HÆ°á»›ng Dáº«n Cháº¡y

### YÃªu Cáº§u TiÃªn Quyáº¿t

1. **Chroma DB Ä‘ang cháº¡y**:

```bash
docker run -p 8000:8000 chromadb/chroma
```

2. **ÄÃ£ upload documents vÃ o collection**:

```bash
# Sá»­ dá»¥ng API endpoint Ä‘á»ƒ upload
POST http://localhost:3000/api/v1/rag/upload
Content-Type: multipart/form-data

{
  "files": [documents],
  "collectionName": "heritage_documents"
}
```

3. **ÄÃ£ config .env vá»›i Naver API keys**:

```env
NAVER_EMBEDDING_API_URL=https://clovastudio.stream.ntruss.com/testapp/v1/api-tools/embedding/clir-emb-dolphin/04e3d63176554bbeb55d0f72f2b5e96a
NAVER_EMBEDDING_API_KEY=your_embedding_key

NAVER_CHAT_COMPLETION_API_URL=https://clovastudio.stream.ntruss.com/testapp/v1/chat-completions/HCX-DASH-001
NAVER_CHAT_COMPLETION_API_KEY=your_chat_key
NAVER_CHAT_APIGW_KEY=your_apigw_key
NAVER_CHAT_REQUEST_ID=your_request_id
```

---

### Vá»‹ TrÃ­ File benchmark_data.json

**Äáº·t file á»Ÿ thÆ° má»¥c gá»‘c cá»§a project** (cÃ¹ng cáº¥p vá»›i package.json):

```
heritage-naver-api/
â”œâ”€â”€ benchmark_data.json  â† ÄÃ‚Y
â”œâ”€â”€ package.json
â”œâ”€â”€ src/
â””â”€â”€ ...
```

Hoáº·c báº¡n cÃ³ thá»ƒ Ä‘áº·t á»Ÿ báº¥t ká»³ Ä‘Ã¢u vÃ  chá»‰ Ä‘á»‹nh Ä‘Æ°á»ng dáº«n vá»›i `--input`:

```bash
npm run benchmark -- --input /path/to/my_data.json
```

---

### CÃ¡c Lá»‡nh Cháº¡y

#### 1. Cháº¡y Benchmark CÆ¡ Báº£n (Máº·c Äá»‹nh)

```bash
npm run benchmark
```

Hoáº·c:

```bash
npx babel-node src/benchmark/runBenchmark.js
```

**Máº·c Ä‘á»‹nh**:

- Input: `benchmark_data.json` (thÆ° má»¥c gá»‘c)
- Output: `benchmark_results.json` (thÆ° má»¥c gá»‘c)
- Collection: `heritage_documents`
- Top-K: 5 documents
- Mode: Real API calls

---

#### 2. Cháº¡y Vá»›i Custom Input/Output Files

```bash
npm run benchmark -- --input my_test_data.json --output my_results.json
```

---

#### 3. Cháº¡y Vá»›i Collection KhÃ¡c

```bash
npm run benchmark -- --collection my_custom_collection --topk 10
```

---

#### 4. Cháº¡y Mock Mode (KhÃ´ng Gá»i API)

Há»¯u Ã­ch khi:

- ChÆ°a cÃ³ Naver API keys
- Collection chÆ°a cÃ³ documents
- Muá»‘n test logic mÃ  khÃ´ng tá»‘n API quota

```bash
npm run benchmark -- --mock
```

---

#### 5. Xem HÆ°á»›ng Dáº«n

```bash
npm run benchmark -- --help
```

---

### ThÃªm Script vÃ o package.json (Náº¿u ChÆ°a CÃ³)

Má»Ÿ file `package.json` vÃ  thÃªm vÃ o pháº§n `scripts`:

```json
{
  "scripts": {
    "benchmark": "babel-node src/benchmark/runBenchmark.js",
    "benchmark:mock": "babel-node src/benchmark/runBenchmark.js --mock"
  }
}
```

---

## ğŸ“Š Output Máº«u

### Console Output

```
================================================================================
ğŸš€ RAG BENCHMARK SYSTEM
================================================================================

ğŸ“‹ Configuration:
   Input File: benchmark_data.json
   Output File: benchmark_results.json
   Collection: heritage_documents
   Top-K: 5
   Mock Mode: No (using real API)
================================================================================

ğŸ” Checking Chroma collection status...
âœ… Collection "heritage_documents" found with 150 documents

ğŸ“‚ Loading benchmark data from: benchmark_data.json
âœ… Loaded 10 test cases

ğŸ”„ Running Benchmark...
Progress: â¬›â¬›â¬›â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ 30% (3/10) [Elapsed: 5.2s | ETA: 12.1s]

================================================================================
ğŸ“Š BENCHMARK SUMMARY
================================================================================

Overall Metrics:
  BLEU Score:              0.6234 ğŸŸ¢
  ROUGE-L Score:           0.7128 ğŸŸ¢
  Cosine Similarity:       0.6891 ğŸŸ¢
  Semantic Similarity:     0.7456 ğŸŸ¢
  Average Execution Time:  1.8s

Quality Distribution:
  ğŸŸ¢ Excellent (â‰¥0.8): 3 (30%)
  ğŸŸ¡ Good (0.6-0.8):   5 (50%)
  ğŸŸ  Fair (0.4-0.6):   2 (20%)
  ğŸ”´ Poor (<0.4):      0 (0%)

================================================================================
ğŸ“‹ DETAILED RESULTS
================================================================================

â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ID â”‚ Question (truncated)           â”‚ BLEU â”‚ ROUGE-L â”‚ Cosine â”‚ Semantic â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ q1 â”‚ NhÃ£ nháº¡c cung Ä‘Ã¬nh Huáº¿ lÃ ...  â”‚ 0.75 â”‚ 0.82    â”‚ 0.78   â”‚ 0.85     â”‚
â”‚ q2 â”‚ Di sáº£n Vá»‹nh Háº¡ Long Ä‘Æ°á»£c...   â”‚ 0.68 â”‚ 0.71    â”‚ 0.65   â”‚ 0.73     â”‚
â”‚ q3 â”‚ HÃ¡t xoan cÃ³ nguá»“n gá»‘c tá»«...   â”‚ 0.55 â”‚ 0.64    â”‚ 0.61   â”‚ 0.68     â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

================================================================================
âš ï¸  WORST PERFORMING QUESTIONS
================================================================================

âŒ Question q5 (Average: 0.42)
   "Nghá»‡ thuáº­t Äá»n ca tÃ i tá»­ Nam Bá»™ cÃ³ Ä‘áº·c Ä‘iá»ƒm gÃ¬?"

   Issues:
   - âš ï¸  Retrieved context may not be highly relevant (cosine < 0.5)
   - âš ï¸  Generated answer is too short (< 50 chars)
   - âš ï¸  Low semantic similarity (< 0.5)

   Metrics:
   - BLEU: 0.35 ğŸ”´
   - ROUGE-L: 0.48 ğŸŸ 
   - Cosine: 0.44 ğŸŸ 
   - Semantic: 0.41 ğŸ”´

================================================================================
ğŸ’¡ IMPROVEMENT SUGGESTIONS
================================================================================

ğŸ” Retrieval Issues (2 questions):
   - Consider using better embedding models
   - Increase topK parameter from 5 to 10
   - Review document chunking strategy
   - Affected questions: q5, q8

ğŸ¤– Generation Issues (3 questions):
   - Improve prompt engineering for Naver Chat API
   - Add more context to generation phase
   - Review ground truth quality
   - Affected questions: q3, q5, q7

ğŸ“Š Overall Recommendations:
   âœ“ Average score is Good (0.62), but has room for improvement
   âœ“ Focus on questions with cosine similarity < 0.5
   âœ“ Consider fine-tuning retrieval parameters

ğŸ’¾ Saving results to: benchmark_results.json
âœ… Results saved successfully!

================================================================================
ğŸ‰ BENCHMARK COMPLETED SUCCESSFULLY!
================================================================================

ğŸ“Š Summary:
   Total Questions: 10
   Successful: 10
   Failed: 0

ğŸ“ Output files:
   Results: benchmark_results.json
```

---

### JSON Output (benchmark_results.json)

```json
{
  "results": [
    {
      "id": "q1",
      "question": "NhÃ£ nháº¡c cung Ä‘Ã¬nh Huáº¿ lÃ  di sáº£n vÄƒn hÃ³a phi váº­t thá»ƒ nÃ o cá»§a Viá»‡t Nam?",
      "ground_truth": "NhÃ£ nháº¡c cung Ä‘Ã¬nh Huáº¿ lÃ  di sáº£n vÄƒn hÃ³a phi váº­t thá»ƒ Ä‘Æ°á»£c UNESCO cÃ´ng nháº­n nÄƒm 2003...",
      "generated_answer": "NhÃ£ nháº¡c cung Ä‘Ã¬nh Huáº¿ Ä‘Æ°á»£c UNESCO cÃ´ng nháº­n lÃ  Di sáº£n vÄƒn hÃ³a phi váº­t thá»ƒ Ä‘áº¡i diá»‡n cá»§a nhÃ¢n loáº¡i vÃ o nÄƒm 2003...",
      "retrieved_context": [
        {
          "content": "NhÃ£ nháº¡c cung Ä‘Ã¬nh Huáº¿ lÃ  há»‡ thá»‘ng Ã¢m nháº¡c cung Ä‘Ã¬nh...",
          "metadata": {
            "doc_id": "heritage_001",
            "title": "NhÃ£ nháº¡c cung Ä‘Ã¬nh Huáº¿"
          },
          "distance": 0.15
        }
      ],
      "metrics": {
        "bleu": 0.7523,
        "rougeL": 0.8234,
        "cosine": 0.7812,
        "semantic": 0.8456
      },
      "execution_time": 1.823,
      "timestamp": "2024-11-10T10:30:45.123Z"
    }
  ],
  "metadata": {
    "total_questions": 10,
    "successful": 10,
    "failed": 0,
    "average_metrics": {
      "bleu": 0.6234,
      "rougeL": 0.7128,
      "cosine": 0.6891,
      "semantic": 0.7456
    },
    "config": {
      "collection": "heritage_documents",
      "topK": 5,
      "mockMode": false
    },
    "timestamp": "2024-11-10T10:30:50.456Z",
    "export_path": "benchmark_results.json"
  }
}
```

---

## ğŸ”§ Troubleshooting

### Lá»—i: "Collection does not exist"

**NguyÃªn nhÃ¢n**: ChÆ°a upload documents vÃ o Chroma
**Giáº£i phÃ¡p**:

1. Cháº¡y láº¡i vá»›i `--mock` Ä‘á»ƒ test logic
2. Hoáº·c upload documents qua API endpoint `/api/v1/rag/upload`

### Lá»—i: "Naver API connection failed"

**NguyÃªn nhÃ¢n**: API keys chÆ°a Ä‘Æ°á»£c config hoáº·c sai
**Giáº£i phÃ¡p**:

1. Kiá»ƒm tra file `.env`
2. Cháº¡y vá»›i `--mock` náº¿u chÆ°a cÃ³ API keys

### Lá»—i: "Cannot find module chromadb"

**NguyÃªn nhÃ¢n**: Package chromadb chÆ°a Ä‘Æ°á»£c cÃ i
**Giáº£i phÃ¡p**:

```bash
npm install chromadb
```

### Lá»—i: "Chroma DB connection refused"

**NguyÃªn nhÃ¢n**: Chroma DB chÆ°a cháº¡y
**Giáº£i phÃ¡p**:

```bash
docker run -p 8000:8000 chromadb/chroma
```

---

## ğŸ“ˆ Giáº£i ThÃ­ch Metrics

### BLEU (Bilingual Evaluation Understudy)

- **Range**: 0.0 - 1.0 (cÃ ng cao cÃ ng tá»‘t)
- **Ã nghÄ©a**: Äo Ä‘á»™ chÃ­nh xÃ¡c cá»§a n-grams (1-gram, 2-gram, 3-gram, 4-gram)
- **Ãp dá»¥ng**: So sÃ¡nh tá»«ng tá»« vÃ  cá»¥m tá»« giá»¯a generated answer vÃ  ground truth
- **Threshold**:
  - â‰¥ 0.8: Excellent
  - 0.6-0.8: Good
  - 0.4-0.6: Fair
  - < 0.4: Poor

### ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation - Longest Common Subsequence)

- **Range**: 0.0 - 1.0 (cÃ ng cao cÃ ng tá»‘t)
- **Ã nghÄ©a**: Äo Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng dá»±a trÃªn chuá»—i con chung dÃ i nháº¥t
- **Ãp dá»¥ng**: ÄÃ¡nh giÃ¡ cáº¥u trÃºc vÃ  thá»© tá»± tá»«
- **Threshold**: TÆ°Æ¡ng tá»± BLEU

### Cosine Similarity (TF-IDF)

- **Range**: 0.0 - 1.0 (cÃ ng cao cÃ ng tá»‘t)
- **Ã nghÄ©a**: Äo Ä‘á»™ tÆ°Æ¡ng tá»± vá» máº·t tá»« vá»±ng (TF-IDF vectors)
- **Ãp dá»¥ng**: ÄÃ¡nh giÃ¡ ná»™i dung vÃ  tá»« khÃ³a quan trá»ng
- **Threshold**: TÆ°Æ¡ng tá»± BLEU

### Semantic Similarity (Embedding-based)

- **Range**: 0.0 - 1.0 (cÃ ng cao cÃ ng tá»‘t)
- **Ã nghÄ©a**: Äo Ä‘á»™ tÆ°Æ¡ng tá»± vá» máº·t ngá»¯ nghÄ©a (embedding vectors)
- **Ãp dá»¥ng**: ÄÃ¡nh giÃ¡ Ã½ nghÄ©a tá»•ng thá»ƒ, khÃ´ng phá»¥ thuá»™c tá»« vá»±ng cá»¥ thá»ƒ
- **Threshold**: TÆ°Æ¡ng tá»± BLEU

---

## ğŸ¯ Best Practices

1. **Táº¡o test cases cháº¥t lÆ°á»£ng**:

   - Ground truth pháº£i chÃ­nh xÃ¡c vÃ  Ä‘áº§y Ä‘á»§
   - Questions nÃªn Ä‘a dáº¡ng (easy, medium, hard)
   - Cover nhiá»u topics khÃ¡c nhau

2. **Tune parameters**:

   - Báº¯t Ä‘áº§u vá»›i `topK=5`, tÄƒng lÃªn náº¿u retrieval kÃ©m
   - Monitor execution time vs quality tradeoff

3. **PhÃ¢n tÃ­ch káº¿t quáº£**:

   - Focus vÃ o worst performers Ä‘á»ƒ cáº£i thiá»‡n
   - Xem suggestions Ä‘á»ƒ biáº¿t nÆ¡i cáº§n optimize
   - Compare metrics giá»¯a cÃ¡c láº§n cháº¡y

4. **Iterate vÃ  improve**:
   - Cháº¡y benchmark thÆ°á»ng xuyÃªn sau khi thay Ä‘á»•i code
   - Track metrics theo thá»i gian
   - A/B test giá»¯a cÃ¡c chiáº¿n lÆ°á»£c khÃ¡c nhau

---

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o

- **BLEU Paper**: Papineni et al. (2002) "BLEU: a Method for Automatic Evaluation of Machine Translation"
- **ROUGE Paper**: Lin (2004) "ROUGE: A Package for Automatic Evaluation of Summaries"
- **Chroma DB Docs**: https://docs.trychroma.com/
- **Naver Cloud AI**: https://www.ncloud.com/product/aiService/clovaStudio

---

## âœ‰ï¸ Support

Náº¿u gáº·p váº¥n Ä‘á», hÃ£y kiá»ƒm tra:

1. Logs trong console output
2. File `benchmark_results.json` Ä‘á»ƒ xem chi tiáº¿t errors
3. Network connectivity Ä‘áº¿n Chroma DB vÃ  Naver API

---

**ChÃºc báº¡n benchmark thÃ nh cÃ´ng! ğŸš€**
